---
title: 'Research'
date: 2018-02-22T17:01:34+07:00
layout: page
bodyClass: page-about
---



## 研究内容

- 航空、航天机器人：
柔性机械臂的控制、空间机器人操作的半物理仿真、基于半物理仿真的飞行模拟


- 机器人柔性体操作：
自动配线机器人、柔性工件的自动装配
 


- 服务机器人：
室内自主装修机器人、建筑机器人



## 实验视频
### 柔软体组项目视频
**机械臂操作无标记的纸片对齐纸片中线和盒子边缘**
<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"  src="//player.bilibili.com/player.html?aid=376526176&bvid=BV1Ao4y1X7mt&cid=365353139&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  > </iframe>
</div> 
  
**Vision Based Cable Assembly in Constrained Environment**
<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?aid=504103592&bvid=BV1ag411T7J7&cid=365309084&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" > </iframe>  
</div> 
  
**Manipulation and Deformation Control of Deformable Objects Based on Friction Control**
<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?aid=461547644&bvid=BV1pL411W7f8&cid=365350084&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" > </iframe>
</div>  
  
### 建筑组项目视频
**Automatic Wall Sanding Based on UR Manipulator**
<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?aid=504071890&bvid=BV1Ag411g7K7&cid=365351415&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" > </iframe>
</div> 
  
**Vision Based Automatic Putty Applying**
<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?aid=504073715&bvid=BV1wg411g7xK&cid=365347557&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" > </iframe>
</div> 
  
**Development of Macro-Micro Robot with a Compliant End Effector for Putty Applyin**
<div style="position: relative; padding: 30% 45%;">
<iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="//player.bilibili.com/player.html?aid=888969315&bvid=BV1nK4y1M7SM&cid=364457792&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" > </iframe>
</div> 
  
## 测试
<h2 id="soft-manipulation">机器人柔性体操作</h2>
<p>在柔软体操作领域~~~</p>
<html>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:0px;width:30%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none" align="center">
             <!-- <img src="../images/research_imgs/auto_driving11.gif" alt="hpp" style="border-style: none; margin: 20px 0px -5px 0px" width="100%" />-->
            <!--   <img src="../images/research_imgs/auto_driving12.gif" alt="hpp" style="border-style: none" width="100%" />-->
             <iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"  src="//player.bilibili.com/player.html?aid=376526176&bvid=BV1Ao4y1X7mt&cid=365353139&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  > </iframe>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none">
                <papertitle><strong>机械臂操作无标记的纸片对齐纸片中线和盒子边缘</strong>
                </papertitle> 
              <br />
              Due to noisy pixels caused by dynamic objects in the outdoor scenes, visual odometry is heavily affected under such dynamic environments. We propose a confidence-based unsupervised visual odometry model to fully leverage the similarity and consistency of correspondent pixels, improving the robustness to dynamic objects (<a href="https://ieeexplore.ieee.org/document/9345430">TITS 2021</a>, <a href="https://ieeexplore.ieee.org/document/8968524">IROS 2019</a>). Since motion segmentation is also important to dynamic scenes, we extract features from sequential depth maps through GRU to segment dynamic objects from visual perception, which improves the odometry performance on KITTI dataset and real-site application (<a href="https://link.springer.com/chapter/10.1007/978-3-030-29911-8_39">PRICAI 2019</a>). For LiDAR-based odometry, we introduce a novel 3D point cloud learning model, named PWCLO-Net, using hierarchical embedding mask optimization. It outperforms all recent learning-based methods and outperforms the geometry-based approach, LOAM with mapping optimization, on most sequences of the KITTI odometry dataset (<a href="https://arxiv.org/abs/2012.00972">CVPR 2021</a>).            
            </td>
          </tr>
    </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:0px;width:30%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none">
              <img src="../images/research_imgs/auto_driving2.gif" alt="hpp" style="border-style: none" width="100%" />
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none">
                <papertitle><strong>Large-scale Depth Estimation and Mapping</strong>
                </papertitle> 
              <br />
              In autonomous driving, the perceptions of depth, optical flow and camera ego-motion are the fundamental abilities for many high-level tasks, such as SLAM, obstacle avoidance and navigation. We studied the problem of pixel mismatch caused by occlusion and illumination change, the problem of depth degradation in long-term training, and the problem of optical flow estimation in occlusion area. By analyzing tasks above, pixels in the middle frame are modeled into three parts: the rigid region, the non-rigid region, and the occluded region. After explicitly occlusion handling, several strategies are proposed to handle the problems in different regions, such as the less-than-mean mask, the maximum normalization, and the consistency of depth-pose and optical flow. Through the proposed strategies, the performances on the three tasks are improved, which is demonstrated in public autonomous driving datasets. (<a href="https://ieeexplore.ieee.org/document/9152137">TITS 2020</a>, <a href="https://ieeexplore.ieee.org/document/8793622">ICRA 2019</a>)             
            </td>
          </tr>
    </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:0px;width:30%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none" align="center">
              <img src="../images/research_imgs/auto_driving31.gif" alt="hpp" style="border-style: none; margin: 20px 0px -20px 0px" width="100%" />
              <img src="../images/research_imgs/auto_driving32.gif" alt="hpp" style="border-style: none" width="100%" />
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none">
                <papertitle><strong>Long-term Loop Closure and Relocalization</strong>
                </papertitle> 
              <br />
              The changing environments pose great challenge on the long-term SLAM algorithms, especially for loop closure and relocalization. We A self-supervised representation learning is proposed to extract domain-invariant features through multi-domain image translation by introducing feature consistency loss. Besides, a novel gradient-weighted similarity activation mapping loss is incorporated for high-precision localization (<a href="https://www.xml-journal.cn/pdfonline/pdf/contentview/cd9ce590ac6ada053bed6ff92763c14d7b0b939c0e87e3a716a830461895b888/JAS-2020-1028.pdf">JAS 2021</a>, <a href="https://ieeexplore.ieee.org/document/8968047">IROS 2019</a>). To leverage the high-quality virtual ground truths without any human effort, we propose a novel multi-task architecture to fuse the geometric and semantic information into the latent embedding representation through syn-to-real domain adaptation(<a href="https://ieeexplore.ieee.org/document/9296559">TIP 2020</a>). For the large-scale point cloud from LiDAR, we propose a novel discriminative and generalizable global descriptor to represent the large-scale outdoor scene, which reveal the continuous latent embedding feature space for place recognition and loop closure. Based on LPD-Net, point cloud registration is implemented for 6-DoF pose regression and relocalization after loop closure (<a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_LPD-Net_3D_Point_Cloud_Learning_for_Large-Scale_Place_Recognition_and_ICCV_2019_paper.html">ICCV 2019</a>, <a href="https://ieeexplore.ieee.org/document/8967875">IROS 2019</a>, <a href="https://arxiv.org/pdf/2011.14579.pdf">IROS 2020</a>).             
            </td>
          </tr>
    </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:0px;width:30%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none" align="center">
              <img src="../images/research_imgs/auto_driving4.gif" alt="hpp" style="border-style: none" width="100%" />
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle;border-left-style:none;border-bottom-style:none;border-top-style:none;border-right-style:none">
                <papertitle><strong>Unmanned Delivery Robot</strong>
                </papertitle> 
              <br />
              Cooperating with Vipshop, we proposed a multi-sensor fusion based unmanned system with autonomous navigation, localization, planning and control algorithm to solve the last-mile delivery problem in the logistics parks. Our unmanned system integrates multi-sensor fusion based SLAM, multi-model perception, dynamic path planning, obstacle avoidance algorithm and motion control algorithms, achieving high-precision mapping, localization, perception, navigation and obstacle avoidance under complex environments. The system has been validated on multiple platforms and accurate vision-based navigation, localization and fixed parking tasks have been accomplished in the large-scale challenging outdoor environments. And it has been successfully applied to the industry field of logistics and distribution and the trial operation in the SJTU campus and Vipshop headquarter has been completed as well. The single delivery path is more than one kilometer long with satisfying operation effects, and the cumulative delivery has been about thousands of express items.             
            </td>
          </tr>
    </table>
</html>




